---
title: "Práctica 2: Limpieza y Análisis de los datos"
author: "Brais Santos Negreira / Jorge Álvarez Gracia"
date: "Estudio del riesgo de sufrir enfermedades cardíacas. Diciembre 2021"
output: 
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: PEC-header.html
  word_document: default
  pdf_document:
    highlight: zenburn
    toc: yes
---

******

# Descripción del dataset


## Introducción

Este proyecto corresponde a la asignatura de Tipología y ciclo de vida de los datos del máster oficial de Ciencia de de datos de la Universidad Oberta de Cataluña UOC. En el pretendemos afianzar todos los conceptos estudiados en la segunda parte de la asignatura, limpieza y análisis de los datos. Para ello, en primer lugar deberemos elegir un dataset donde a través de la información recogida seamos capaces de llevar a cabo los análisis estudiados en la asignatura, así cómo obtener unas conclusiones de los análisis interesantes. 

## Elección del DataSet

Para ello y después de analizar los distintos dataset disponibles en la plataforma kaggle, hemos optado por el conjunto de datos disponible en el siguiente enlace: https://www.kaggle.com/fedesoriano/heart-failure-prediction.

## Importancia y objetivos de los análisis

El termino cardiopatía se refiere a cualquier enfermedad del corazón independientemente de su naturaleza . Según la OMS las Enfermedades cardiovasculares (ECV) son un conjunto de trastornos al corazón y de los vasos sanguíneos, siendo la principal causa de defunción en el mundo.

Como datos llamativos:

• 17,5 millones de personas murieron por enfermedades cardiovasculares en el mundo.

• 80% de los infartos de miocardio y de los AVC prematuros son prevenibles.

• Más del 75% de las muertes causadas por ECV se producen en países de ingresos medios y bajos.

En España y como podemos leer en la web de la Sociedad Española de Cardiología (SEC): La enfermedad cardiovascular continúa siendo la primera causa de muerte en nuestro país, con 120.859 fallecimientos registrados al año, según los últimos datos del Instituto Nacional de Estadística (2018). 

Si nos fijamos en el segundo punto de las afirmaciones de la OMS, el 80% de los infartos se pueden prevenir. Esto significa que con un estudio pormenorizado de los factores de riesgo, utilizando la cantidad de datos médicos existentes y modelos de machine learning, estas causas de defunción se podrían disminuir considerablemente pudiendo prever con antelación que grupos de población con sus respectivas características médicas van a tener más riesgos que otros.

En definitiva, conocer aquellas variables que más influyen en el riesgo de padecer enfermedades cardiovasculares y ser capaces de obtener modelos capaces de predecir si una persona va a sufrir una enfermedad de este tipo o no puede salvar vidas.

Por ello, podemos considerar la predicción de enfermedades cardiovasculares como uno de los objetivos más importantes de la minería de datos y del machine learning dentro de la medicina.

## Objetivos

Como hemos comentado anteriormente, el objetivo de esta práctica es afianzar los conceptos teóricos vistos en la asignatura a la vez que obtenemos información relevante y análisis 
de aquellas variables que afectan al riesgo de padecer enfermedades cardíacas. 

En concreto, llevaremos a cabo los distintas técnicas para responder a las siguientes preguntas:




#  Integración y selección de los datos de interés a analizar




******
## Carga del dataset

******

Se lee el fichero "heart_in.csv".

```{r,eval=TRUE,echo=TRUE}

heart <- read.csv("../data/heart_in.csv",sep=",")

```


## Explicación de las variables

A continuación, obtenemos el listado de variables:

```{r,eval=TRUE,echo=TRUE}

str(heart)

```

<li> __Age__ : edad del paciente.</li> 
<li> __Sex__ : sexo del paciente. </li> 
<li> __ChestPainType__ : Tipo de dolor del pecho. </li>
<li> __RestingBP__ : presión arterial en reposo (sistólica). </li>
<li> __Cholesterol__ : colesterol sérico. </li>
<li> __FastingBS__ : azúcar en sangre en ayunas.</li>
<li> __RestingECG__: resultados del electrocardiograma en reposo. </li>
<li> __MaxHR__: frecuencia cardíaca máxima alcanzada. </li>
<li> __ExerciseAngina__: angina inducida por el ejercicio. </li>
<li> __Oldpeak__ : ejercicio inducido por la depresión (ST) en relación con el descanso. </li>
<li> __ST_Slope__ : pendiente del segmento ST del ejercicio pico. </li>
<li> __HeartDisease__   : clase de salida. </li>



Vamos a realizar una explicación más extensa de cada variable según los criterios médicos para conocer estos conceptos y su influencia respecto a la hora de sufrir una enfermedad cardíaca.

**Age** : La edad es el factor de riesgo más importante en el desarrollo de enfermedades cardiovasculares o cardíacas, según la SEC, con aproximadamente un triple de riesgo con cada década de vida. Las vetas de grasa coronarias pueden comenzar a formarse en la adolescencia. Se estima que el 82 por ciento de las personas que mueren de enfermedad coronaria tienen 65 años o más. Simultáneamente, el riesgo de accidente cerebrovascular se duplica cada década después de los 55 años.

**Sex**: Los hombres tienen un mayor riesgo de enfermedad cardíaca que las mujeres premenopáusicas. Una vez pasada la menopausia, se ha argumentado que el riesgo de una mujer es similar al del hombre, aunque datos más recientes de la OMS y la ONU lo niegan. Si una mujer tiene diabetes, tiene más probabilidades de desarrollar una enfermedad cardíaca que un hombre con diabetes.

**ChestPainType**: la angina o dolor de pecho es un malestar que se produce cuando el músculo cardíaco no recibe suficiente sangre rica en oxígeno.

**RestingBP**: la presión arterial alta puede dañar las arterias que alimentan el corazón. La presión arterial alta puede ocurrir debido a distintas afecciones, como la obesidad, el colesterol alto o la diabetes.

**Cholesterol**: Un nivel alto de colesterol unido a lipoproteínas de baja densidad (LDL) (el colesterol “malo”) ayuda a que las arterias se estrechen. Un nivel alto de triglicéridos, un tipo de grasa en la sangre, también aumenta el riesgo de un ataque cardíaco. Sin embargo, un nivel alto de colesterol de lipoproteínas de alta densidad (HDL) (el colesterol “bueno”) reduce el riesgo de sufrir un ataque cardíaco.

**FastingBS**: No producir suficiente hormona secretada por el páncreas (insulina) o no responder a la insulina de manera adecuada hace que los niveles de azúcar en sangre del cuerpo aumenten, lo que aumenta el riesgo de un ataque cardíaco.

**RestingECG**: Para las personas con bajo riesgo de enfermedad cardiovascular, la USPSTF concluye con certeza moderada que los daños potenciales de la detección con ECG en reposo o de ejercicio igualan o superan los beneficios potenciales. Para las personas con riesgo intermedio a alto, la evidencia actual es insuficiente para evaluar el balance de beneficios y daños del cribado.

**MaxHR**: Se ha demostrado que un aumento de la frecuencia cardíaca de 10 latidos por minuto se asoció con un aumento del riesgo de muerte cardíaca en al menos un 20%.

**ExerciseAngina**: Angina inducida por una actividad física.

**Oldpeak**: el infarto de miocardio con elevación del segmento ST es una emergencia médica producida por la formación de un trombo sobre una placa rota de aterosclerosis que ocluye la circulación coronaria del músculo cardíaco.

**ST_Slope**: pendiente del segmento ST. 

Se muestra un resumen de las variables.

```{r,eval=TRUE,echo=TRUE}

summary(heart)

```

A partir de esto, se puede concluir con que hay 12 variables y 918 observaciones. También se aprecia que en el dataset hay 3 tipos de datos:

<li> Variables numéricas: Oldpeak. </li>
<li> Variables de tipo carácter: Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope. </li>
<li> Variables de tipo entero: Age, RestingBP, Cholesterol, FastingBS, MaxHR, HeartDisease. </li>


# Limpieza y preprocesado de datos

## Conversión y estandarización

Pasamos los valores int a numeric y los char a factor.

```{r,eval=TRUE,echo=TRUE}

heart$Age <- as.numeric(heart$Age)
heart$Sex <- as.factor(heart$Sex)
heart$ChestPainType <- as.factor(heart$ChestPainType)
heart$RestingBP <- as.numeric(heart$RestingBP)
heart$Cholesterol <- as.numeric(heart$Cholesterol)
heart$FastingBS <- as.numeric(heart$FastingBS)
heart$RestingECG  <- as.factor(heart$RestingECG)
heart$MaxHR <- as.numeric(heart$MaxHR)
heart$ExerciseAngina <- as.factor(heart$ExerciseAngina)
heart$ST_Slope <- as.factor(heart$ST_Slope)
heart$HeartDisease <- as.numeric(heart$HeartDisease)

```


## Estudio de outliers

En este apartado, se va a mostrar el diagrama de cajas de cada una de las variables numéricas para comprobar si hay posibles outliers.

__Age__:

Se aprecia que en la variable "Age" no hay ningún valor extremo.
```{r,eval=TRUE,echo=TRUE}

boxplot(heart$Age,main="Edad del paciente")

```

```{r,eval=TRUE,echo=TRUE}

boxplot.stats(heart$Age)$out

```


__RestingBP__:

En el campo "RestingBP" se observa algunos outliers, pero no se eliminarán debido a que la presión arterial sistólica puede tener valores superiores a 180 (esto sería una crisis de hipertensión).

```{r,eval=TRUE,echo=TRUE}

boxplot(heart$RestingBP,main="Presión arterial en reposo")

```


```{r,eval=TRUE,echo=TRUE}

boxplot.stats(heart$RestingBP)$out

```

__Cholesterol__:

En esta variable se aprecian una serie de valores extremos, pero por el contrario no es extraño ya que un colesterol de 603 se puede tener (https://www.tuotromedico.com/parametros/colesterol-en-sangre-alto.htm), aunque es un valor muy elevado. También se puede tener el valor 100 que es óptimo.

```{r,eval=TRUE,echo=TRUE}

boxplot(heart$Cholesterol,main="Colesterol")

```


```{r,eval=TRUE,echo=TRUE}

boxplot.stats(heart$Cholesterol)$out

```


__MaxHR__ :

Se puede apreciar que hay dos valores extremos: 60 y 63. Esas frecuencias cardíacas __máximas__ son muy bajas y por tanto lo podemos detectar como valores extraños. 

```{r,eval=TRUE,echo=TRUE}

boxplot(heart$MaxHR,main="Frecuencia cardíaca máxima alcanzada")

```

```{r,eval=TRUE,echo=TRUE}

boxplot.stats(heart$MaxHR)$out

```
A continuación, sustituimos esos valores por NA y luego lo trataremos mediante Knn.

```{r,eval=TRUE,echo=TRUE}

posMaxHR <- which(heart$MaxHR==60 | heart$MaxHR==63)


heart$MaxHR[posMaxHR] <- NA


```

Se aplica kNN para ponerle un valor en función de sus vecinos más cercanos.

```{r,eval=TRUE,echo=TRUE}
library(VIM)

MaxHR_new <- kNN(heart,variable="MaxHR",dist_var =c("Age","Sex","ChestPainType","FastingBS","RestingECG","ExerciseAngina","ST_Slope","HeartDisease"),k=3)
MaxHR_new <- MaxHR_new$MaxHR

heart$MaxHR <- MaxHR_new


```

Se comprueba  que en esa posición hay otro valor diferente.

```{r,eval=TRUE,echo=TRUE}

heart$MaxHR[posMaxHR]


```



## Estudio de Valores nulos

Se aprecia que no hay ningún valor NA. 

```{r,eval=TRUE,echo=TRUE}

colSums(is.na(heart))

```

A continuación, comprobaremos si algún campo tiene un valor 0.

```{r,eval=TRUE,echo=TRUE}

colSums(heart == 0)

```

Viendo esto, se puede decir que no tiene sentido que los campos RestingBP y Cholesterol tengan ceros. En este caso, podemos hablar de información perdida.

```{r,eval=TRUE,echo=TRUE}

posRestingBP <- which(heart$RestingBP == 0)

posCholesterol <- which(heart$Cholesterol == 0)

heart$RestingBP[posRestingBP] <- NA

heart$Cholesterol[posCholesterol] <- NA


```

Para esos 0, hemos metido NA ya que es un valor desconocido y ahora miraremos mediante KNN como tratar esos datos.

Para solucionar estos valores perdidos, vamos a hacer uso de la función KNN.

El algoritmo KNN o imputación del vecino más cercano es un algoritmo supervisado cuyo objetivo es que a partir de un juego de datos inicial clasificar correctamente las instancias nuevas. en nuestro caso los valores nulos. 

KNN buscará las distancias entre una consulta y todos los ejemplos en los datos, seleccionando el número especificado ejemplos (K) más cercanos al registro que queremos clasificar y seguidamente elegirá el valor más frecuente


```{r,eval=TRUE,echo=TRUE}



RestingBP_new <- kNN(heart,variable="RestingBP",dist_var =c("Age","Sex","ChestPainType","FastingBS","RestingECG","ExerciseAngina","ST_Slope","HeartDisease"),k=3)
RestingBP_new <- RestingBP_new$RestingBP

heart$RestingBP <- RestingBP_new

Cholesterol_new <- kNN(heart,variable="Cholesterol",dist_var =c("Age","Sex","ChestPainType","FastingBS","RestingECG","ExerciseAngina","ST_Slope","HeartDisease"),k=3)
Cholesterol_new <- Cholesterol_new$Cholesterol

heart$Cholesterol <- Cholesterol_new





```

Se comprueba que ya no hay valores perdidos en esas posiciones:

```{r,eval=TRUE,echo=TRUE}


heart$RestingBP[posRestingBP]
heart$Cholesterol[posCholesterol]



```

Observamos que los valores NA fueron sustituidos por otros en función de las características de los demás atributos.

Finalmente, vemos que ya no hay ningún valor perdido.

```{r,eval=TRUE,echo=TRUE}

colSums(is.na(heart))

```


## PCA: Análisis de componentes principales


Con el PCA lo que se busca es reducir la dimensionalidad. Partimos de una serie de atributos numéricos del dataset “heart.csv” correlacionados y eso lo convertiremos en menos variables sin estar relacionadas entre si, pero queriendo mantener la mayor información posible.

En el dataset "heart.csv", no todos los atributos son numéricos. Por tanto, los convertiremos y lo añadiremos en otro dataset.

```{r,eval=TRUE,echo=TRUE}

heart_num <- heart
heart_num$Sex <- with(heart_num, ifelse(Sex == "M", 0,1))
heart_num$ChestPainType <- with(heart_num, ifelse(ChestPainType == "ASY",0,ifelse(ChestPainType == "ATA",1,ifelse(ChestPainType == "NAP",2,3))))
heart_num$RestingECG <- with(heart_num, ifelse(RestingECG == "LVH",0,ifelse(RestingECG == "Normal",1,2)))
heart_num$ExerciseAngina <- with(heart_num, ifelse(ExerciseAngina == "N", 0,1))
heart_num$ST_Slope <- with(heart_num, ifelse(ST_Slope == "Down", 0,ifelse(ST_Slope == "Flat",1,2)))

```


```{r,eval=TRUE,echo=TRUE}

 pca = prcomp(heart_num, scale = TRUE)

  plot(pca, type="l")

```

En la gráfica se observa cuales son los componentes principales más significativos o con más “peso”. Estos son el PC1 y PC2.

Se muestra un resumen de cada uno de los componentes principales.

```{r,eval=TRUE,echo=TRUE}

summary(pca)

```

Visto la información que da “Proportion of Variance”, el PC1 y PC2 nos dá el 38.07% de la información del dataset.
<br>

La línea roja indica el valor medio de contribución. Podemos decir que aquellas variables superiores a esta linea son importantes a la hora de contribuir a este componente.

```{r,eval=TRUE,echo=TRUE}
library("factoextra")

fviz_contrib(pca, addlabels = TRUE, choice = "var", axes = 1, top = 20)

```



```{r,eval=TRUE,echo=TRUE}


library("factoextra")

fviz_pca_var(pca,
             col.var = "contrib", # color en funcón de la contribución. 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE    
             )


```

Viendo esto, se puede ver como el PC1 le asigna más peso a la variable “HeartDisease” que se mueve por el eje X.
Por otro lado, vemos como el PC2 le asigna más peso a la variable “RestingECG” que se mueve por el eje Y.


Se muestra otro gráfico para que la información quede más clara.

```{r,eval=TRUE,echo=TRUE}

biplot(pca, scale = 0, cex = 0.5, col = c("lightblue", "red"))

```

Se aprecia como la información es bastante dispar entre los diferentes componentes PC1 y PC2. Por lo tanto, decidimos no eliminar ningún campo del dataset ya que podemos considerar que todas las variables son importantes.

## Normalización 

En este apartado, lo que vamos a hacer es normalizar el campo Age usando min-max. Se creará un nuevo campo llamado Age_nor y se añadirá al dataset.

```{r,eval=TRUE,echo=TRUE}

Age_nor <- (2*(heart$Age-min(heart$Age))/(max(heart$Age)-min(heart$Age)))-1
heart <- cbind(heart,Age_nor)

```

Estos datos, están en el intervalo [-1,1]

```{r,eval=TRUE,echo=TRUE}

min(heart$Age_nor)
max(heart$Age_nor)

```

Se normaliza esta variable para acotar los datos en un intervalo más pequeño ya que puede haber edades dispares y también para poder ver si esta variable puede llegar a seguir una distribución normal.

## Discretización de variables

En esta apartado, también usaremos la variable "Age" para discretizarla en diferentes rangos y se añadirá al dataset con el nombre Age_dis.

```{r,eval=TRUE,echo=TRUE}

Age_dist <- cut(heart$Age, breaks = c(0,40,60,80), labels = c("(0-40]","(40,60]","(60,80]"))

heart <- cbind(heart,Age_dist)

```

Se discretiza esta variable para dividir la edad en rangos y  poder realizar los análisis gráficos más eficientemente, y de esta manera comprobar como influirá cada variable a la hora de sufrir una enfermedad cardíaca según los diferentes grupos de edad de la población. 


# Análisis de los datos


## Estudio descriptivo de las variables

Ahora, tenemos el dataset sin valores vacíos así como con las nuevas variables normalizadas y discretizadas.

Por lo tanto, se muestra un resumen del dataset en el cual podemos ver la media, moda, cuartiles y valores mínimos y máximos en el caso de variables numéricas.

Obtenemos las distintas medidas para las variables cuantitativas:

```{r}
id = c(1,4,5,6,8,10)
mean <- as.vector(sapply(heart[,id],mean,na.rm = TRUE))
std <- as.vector(sapply(heart[,id],sd,na.rm = TRUE))
median <- as.vector(sapply(heart[,id],median,na.rm = TRUE))
mean.trim <- as.vector(sapply(heart[,id],mean,na.rm = TRUE,trim = 0.05))
IQR <- as.vector(sapply(heart[,id],IQR,na.rm = TRUE))
mad <- as.vector(sapply(heart[,id],mad,na.rm = TRUE))
```

Creamos dos tablas con las medidas obtenidas anteriormente:
```{r}
library(kableExtra)
kable(data.frame(Variables = names(heart)[id],MEDIA   = mean, MEDIANA   = median, MEDIA_RECORTADA  = mean.trim),
digits = 3, caption = "Estimaciones tendencia central")%>% kable_styling(bootstrap_options = c("striped","hover", "responsive"),position = "left")
```
```{r}
kable(data.frame(Variables = names(heart)[id],SD = std, IQR = IQR, MAD = mad),
digits = 3, caption = "Estimaciones de dispersión")%>% kable_styling(bootstrap_options = c("striped","hover", "responsive"),position = "left")
```


En el caso de los factores, podemos ver el número de ocurrencias que tiene dicho valor.

```{r,eval=TRUE,echo=TRUE}

summary(heart)

```
## Estudio normalidad de las variables

### Comprobación de la normalidad

En este apartado, comprobaremos la normalidad de las variables numéricas no dicotómicas.

__Age__


```{r,eval=TRUE,echo=TRUE}

qqnorm(heart$Age)
qqline(heart$Age)

```

A simple vista, se puede ver que los datos están por la recta menos por los lados por ello no podríamos asumir la normalidad. Tendremos que comprobarlo con el test de shapiro y Lilliefors.

```{r,eval=TRUE,echo=TRUE}

shapiro.test(heart$Age)

```

```{r,eval=TRUE,echo=TRUE}

library(nortest)
lillie.test(heart$Age)

```

Obserando ambos test, se puede apreciar que la variable **Age**no sigue una distribución normal.

__RestingBP__

```{r,eval=TRUE,echo=TRUE}

qqnorm(heart$RestingBP)
qqline(heart$RestingBP)

```

Observando el gáfico la variable **RestingBP**  no sigue una distribucción normal.

__Cholesterol__

```{r,eval=TRUE,echo=TRUE}

qqnorm(heart$Cholesterol)
qqline(heart$Cholesterol)

```

Se aprecia en los extremos puntos muy distantes de la recta. Por ello, concluimos que el campo **Cholesterol** no sigue una distribución normal.

__MaxHR__

```{r,eval=TRUE,echo=TRUE}

qqnorm(heart$MaxHR)
qqline(heart$MaxHR)

```

Se observa que los puntos suelen seguir la recta, menos en loslados.

Para estar más seguros aplicamos los test de shapiro y Lilliefors.

```{r,eval=TRUE,echo=TRUE}

shapiro.test(heart$MaxHR)

```
```{r,eval=TRUE,echo=TRUE}

lillie.test(heart$MaxHR)

```

Según ambos test, la variable **MaxHR** no sigue una distribución normal.

__Oldpeak__

```{r,eval=TRUE,echo=TRUE}

qqnorm(heart$Oldpeak)
qqline(heart$Oldpeak)

```

A simple vista, se observa que muchos puntos están fuera de la recta y con esto podemos decir que no sigue una distribución normal.

__Age_nor__

```{r,eval=TRUE,echo=TRUE}

qqnorm(heart$Age_nor)
qqline(heart$Age_nor)

```

Se aprecia que muchos puntos siguen la recta (similar a la variable Age sin normalizar ).

A continuación, aplicamos los siguientes test:

```{r,eval=TRUE,echo=TRUE}

shapiro.test(heart$Age_nor)

```

```{r,eval=TRUE,echo=TRUE}

lillie.test(heart$Age_nor)

```
Nos da la misma información que la variable Age. Por tanto, podemos decir que no sigue una distribución normal.



## Contraste de hipótesis

### Queremos saber si el colesterol de las mujeres es mayor que el de los hombres

Primero, calculamos dos variables para obtener el colesterol de hombres y mujeres.
```{r,eval=TRUE,echo=TRUE}

colHom <- heart$Cholesterol[heart$Sex=="M"]

colMuj <- heart$Cholesterol[heart$Sex=="F"]

```

Como hipótesis nula, indicamos que el colesterol de las mujeres es igual que al de los hombres.

Como hipótesis alternativa indicamos que el colesterol de las mujeres es mayor que el de los hombres.

$$
\left\{
\begin{array}{ll}
H_{0}: &  \mu_M=\mu_H \\
H_{1}: & \mu_M>\mu_H
\end{array}
\right.\\
\ Hipótesis\ unilateral
$$
Por el teorema del límite central, podemos asumir normalidad, puesto que tenemos una muestra de tamaño grande n y se desea realizar un test sobre la media. Por tanto, aplicamos un test de hipótesis de dos muestras sobre la media. 

Aplicaremos la distribución t, dado que no se conoce la varianza de la población.

Es un test unilateral por la derecha.

Ahora, comprobamos la homocedasticidad.


```{r,eval=TRUE,echo=TRUE}

var.test(colMuj, colHom)

```
Se observa que p-value < 0.05. Por tanto, las varianzas no son iguales.

Calculamos manualmente este constraste:

```{r,eval=TRUE,echo=TRUE}

alfa <- 1-0.95

n1 <- length(colMuj)
n2 <- length(colHom)
s1 <- sd(colMuj)
s2 <- sd(colHom) 

mean1 <- mean(colMuj)
mean2 <- mean(colHom)
tobs <- (mean1-mean2) / (sqrt(s1^2/n1 + s2^2/n2))

df <- ((s1^2/n1 + s2^2/n2)^2) / ((s1^2/n1)^2/(n1-1) + (s2^2/n2)^2/(n2-1))

tcritU <- qt(alfa,df,lower.tail = FALSE) 

pvalue <-pt( tobs,df, lower.tail=FALSE)

paste("Estadistico de contraste: ",tobs)
paste("Valor crítico: ",tcritU)
paste("Valor p: ",pvalue)
paste("Valor de df: ",df)

```

A continuación, aplicamos el test de este contraste de hipótesis para comprobar que nos da lo mismo.

```{r,eval=TRUE,echo=TRUE}

t.test( colMuj,colHom , var.equal=FALSE, alternative = "greater")

```

Se aprecia, que p-value < 0.05. Por tanto, se rechaza la hipótesis nula y se acepta la alternativa de que las mujeres tiene más colesterol que los hombres. 

### Azúcar en sangre es mayor en hombres que en mujeres teniendo ambos enfermedades cardíacas


Primero, calculamos dos variables para obtener las enfermedades cardíacas de hombres y mujeres.
```{r,eval=TRUE,echo=TRUE}

carHom <- heart$FastingBS[heart$Sex=="M" & heart$HeartDisease=="1"]

carMuj <- heart$FastingBS[heart$Sex=="F" & heart$HeartDisease=="1"]

```

Como hipótesis nula, se quiere saber si las enfermedades cardíacas es igual en hombres que en mujeres.

Como hipótesis alternativa se quiere saber si las enfermedades cardíacas es mayor en hombres que en mujeres.

$$
\left\{
\begin{array}{ll}
H_{0}: &  \mu_H=\mu_M \\
H_{1}: & \mu_H>\mu_M
\end{array}
\right.\\
\ Hipótesis\ unilateral
$$

Por el teorema del límite central, podemos asumir normalidad, puesto que tenemos una muestra de tamaño grande n y se desea realizar un test sobre la media. Por tanto, aplicamos un test de hipótesis de dos muestras sobre la media. 

Aplicaremos la distribución t, dado que no se conoce la varianza de la población.

Es un test unilateral por la derecha.

Ahora, comprobamos la homocedasticidad.


```{r,eval=TRUE,echo=TRUE}

var.test(carHom,carMuj)

```
Se observa que p-value > 0.05. Por tanto, las varianzas son iguales.

A continuación, aplicamos el test de este contraste de hipótesis.

```{r,eval=TRUE,echo=TRUE}

alfa <- 1-0.95

n1 <- length(carHom)
n2 <- length(carMuj)
s1 <- sd(carHom)
s2 <- sd(carMuj) 

mean1 <- mean(carHom)
mean2 <- mean(carMuj)


df <- n1+n2-2

S <- sqrt( ( (n1-1)*s1^2 + (n2-1)*s2^2 ) / (n1+n2-2) )

 tobs <- (mean1-mean2) / (S * sqrt(1/n1 + 1/n2))
 tcritL <- qt( alfa/2, n1+n2-2)
 tcritU <- qt( 1-alfa/2, n1+n2-2)
 pvalue <-pt( abs(tobs), df=n1+n2-2, lower.tail=FALSE)


paste("Estadistico de contraste: ",tobs)
paste("Valor crítico: ",tcritU)
paste("Valor p: ",pvalue)
paste("Valor de df: ",df)



```

Aplicamos el test para comprobar si nos da el mismo resultado.

```{r,eval=TRUE,echo=TRUE}

t.test( carHom,carMuj , var.equal=TRUE, alternative = "greater")

```

Se aprecia, que p-value > 0.05. Por tanto, se acepta la hipótesis nula de que los hombres con enfermedad cardíaca tienen igual azúcar en sangre que las mujeres con enfermedad cardíaca.

## Estudio de correlación entre las variables

En este apartado queremos representar la matriz de correlación del dataset.

Utilizamos el dataset utilizado para el análisis anterior de PCA


Calculamos la correlación de las variables usando el método de pearson.

```{r,eval=TRUE,echo=TRUE}

Corelacion <- cor(heart_num,method="pearson")
Corelacion

```

A continuación, representamos la matriz de correlación.

```{r,eval=TRUE,echo=TRUE}
if (!require('corrplot')) install.packages('corrplot'); library('corrplot')
corrplot(Corelacion, method = "number")

```

A partir de aqui, podemos observar lo siguiente: 


<li>Las variables HeartDisease y ST_Slope tienen una correlación moderada directa (0.56). </li>
<li>Las variables Oldpeak y ST_Slope tienen una correlación moderada directa (0.50). </li>
<li> Las variables MaxHR y HeartDisease tienen una correlación débil directa (0.40). </li>

Si observamos la variable que consideramos más importante en nuestro dataset, **HeartDisease**, podemos decir que aquellas variables que tienen  más correlación son: **MaxHR**, **ExerciseAngina**,**Olpeak** y **ST_Slope**. 

Este análisis previo nos servirá para en los modelos de regresión siguientes identificar cuales podrían ser aquellas variables independientes más importantes.  

## Regresión lineal

### Regresión lineal simple

En los modelos de regresión lineal la variable dependiente deberá ser una variable continua por ello consideramos como variable explicativa la variable **MaxHR**.

En primer lugar, realizamos un modelo de regresión lineal simple cuya variable dependiente es MaxHR y independiente es Age.

```{r,eval=TRUE,echo=TRUE}

Model_reg_sim <- lm(MaxHR~Age,data=heart)
summary(Model_reg_sim)

```

Se observa que el coeficiente de determinación ajustado es bajo: 14.6%.

### Regresión lineal múltiple

Se crea un modelo de regresión lineal múltiple con la variable dependiente **MaxHR** y indpendientes: **HeartDisease**, **ST_Slope**, **FastingBS**, **Oldpeak**, **RestingECG**, **Sex**, **Cholesterol**.


```{r,eval=TRUE,echo=TRUE}

Model_reg_mul <- lm(MaxHR~Age+HeartDisease+ST_Slope+FastingBS+Oldpeak+RestingECG+Sex+Cholesterol,data=heart)
summary(Model_reg_mul)

```

Se aprecia un coeficiente de determinación bajo (30%).

Podemos considerar que estos modelos no son validos.


## Regresión logística

### Regresión logística simple

Vamos a crear una regresión logística simple como variable dependiente **HeartDisease** y independiente **MaxHR**.

```{r,eval=TRUE,echo=TRUE}

model_logis1 <- glm(formula=HeartDisease~MaxHR,data=heart,family=binomial())
summary(model_logis1)

```

Calculamos la bondad de ajuste usando el test de hoslem.

```{r,eval=TRUE,echo=TRUE}

if (!require('ResourceSelection')) install.packages('ResourceSelection'); library('ResourceSelection')

hoslem.test(heart$HeartDisease,fitted(model_logis1))

```

Se observa que p-value >0.05. Por tanto, el modelo ajusta bien los datos.

### Regresión logística múltiple

En este modelo queremos conocer la probabilidad de una persona de sufrir una enfermedad cardíaca o no, dependiendo de una serie de variables independientes.

Calculamos una regresión logística múltiple con **HeartDisease** como variable dependiente y **ST_Slope**, **Oldpeak**, **MaxHR**, **ChestPainType**, **RestingBP**, **Age_nor** como variables dependientes

```{r,eval=TRUE,echo=TRUE}

model_logis2 <- glm(formula=HeartDisease~ST_Slope+Oldpeak+MaxHR+ChestPainType+RestingBP+Age_nor,data=heart,family=binomial())
summary(model_logis2)

```

Se aprecia que el AIC es mucho menor y por tanto, el modelo es mejor.

Calculamos la bondad de ajuste:

```{r,eval=TRUE,echo=TRUE}

hoslem.test(heart$HeartDisease,fitted(model_logis2))

```
Se aprecia que el modelo tiene p-value > 0.05. Por tanto, ajusta bien los datos.

Vamos a dividir el conjunto de los datos en un conjunto de entrenamiento y otro de prueba para validar el modelo.

```{r,eval=TRUE,echo=TRUE}

set.seed(42)
index <- sample(1:nrow(heart),size = floor(0.80*nrow(heart)))

train <- heart[index,]
test <- heart[-index,]

```


```{r,eval=TRUE,echo=TRUE}

model_logis3 <- glm(formula=HeartDisease~ST_Slope+Oldpeak+MaxHR+ChestPainType+RestingBP+Age_nor,data=train,family=binomial())
summary(model_logis3)

```


```{r,eval=TRUE,echo=TRUE}

predicion_1 <- ifelse(predict(model_logis3,newdata = test, type="response") >=0.5,1,0)
predicion_1
```

```{r,eval=TRUE,echo=TRUE}

mat_conf_1<-table(test$HeartDisease,Predicted=predicion_1)
mat_conf_1

```

```{r,eval=TRUE,echo=TRUE}
if(!require(gmodels)){
    install.packages('gmodels', repos='http://cran.us.r-project.org')
    library(gmodels)
}

CrossTable(test$HeartDisease,predicion_1,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))

```

```{r,eval=TRUE,echo=TRUE}

print(sprintf("La precisión del modelo es: %.4f %%",100*(158/184)))
```


## Modelos de clasificación

El objetivo de este modelo es clasificar adecuadamente aquellas personas según los datos dados para cada variable si va a sufrir una enfermedad cardíaca o no.


### Preparación de los datos para el modelo

Nos interesa desordenar los datos para poder elegir registros aleatorios. Guardaremos los datos con el nuevo nombre de: **heart_clasification**

```{r,eval=TRUE,echo=TRUE}

set.seed(1)
heart_clasification <- heart[sample(nrow(heart)),]

```

Para la futura evaluación del árbol de decisión, es necesario dividir el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento es el subconjunto del conjunto original de datos utilizado para construir un primer modelo; y el conjunto de prueba, el subconjunto del conjunto original de datos utilizado para evaluar la calidad del modelo.

Lo más correcto será utilizar un conjunto de datos diferente del que utilizamos para construir el árbol, es decir, un conjunto diferente del de entrenamiento. No hay ninguna proporción fijada con respecto al número relativo de componentes de cada subconjunto, pero la más utilizada acostumbra a ser 2/3 para el conjunto de entrenamiento y 1/3, para el conjunto de prueba.

La variable por la que clasificaremos es HeartDisease, para clasificar si existe riesgo de enfermedad cardíaca o no, y que se sitúa en nuestro en la columna, la 12. De esta forma, tendremos un conjunto de datos para el entrenamiento y uno para la validación.
Para clasificar dicha variable utilizaremos tpdas las variables anteriores excepto la edad normalizada y la edad discreta. 

```{r,eval=TRUE,echo=TRUE}

set.seed(666)
y <- heart_clasification[,12] 
X <- heart_clasification[,1:11] 

```

Dividimos el dataset en dos partes diferenciadas, train y test. Podemos crear directamente un rango utilizando el parámetro split_prop

```{r,eval=TRUE,echo=TRUE}

split_prop <- 3 
indexes = sample(1:nrow( heart_clasification), size=floor(((split_prop-1)/split_prop)*nrow(heart_clasification)))
trainx<-X[indexes,]
trainy<-y[indexes]
testx<-X[-indexes,]
testy<-y[-indexes]

```


### Creación del modelo, calidad del modelo y extracción de reglas



```{r,eval=TRUE,echo=TRUE}

if(!require(ggplot2)){
    install.packages('ggplot2', repos='http://cran.us.r-project.org')
    library(ggplot2)
}
if(!require(ggpubr)){
    install.packages('ggpubr', repos='http://cran.us.r-project.org')
    library(ggpubr)
}
if(!require(grid)){
    install.packages('grid', repos='http://cran.us.r-project.org')
    library(grid)
}

if(!require(gridExtra)){
    install.packages('gridExtra', repos='http://cran.us.r-project.org')
    library(gridExtra)
}

if(!require(C50)){
    install.packages('C50', repos='http://cran.us.r-project.org')
    library(C50)
}

library("rpart")       
library("rpart.plot")  

```


```{r,eval=TRUE,echo=TRUE}
if(!require(C50)){
    install.packages('C50', repos='http://cran.us.r-project.org')
    library(C50)
}
trainy = as.factor(trainy)
modelo_clasificacion_C50 <- C50::C5.0(trainx, trainy,rules=TRUE,control = C5.0Control(noGlobalPruning = FALSE) )
summary(modelo_clasificacion_C50)

```

Se observa que los atributos que más se usan en este modelo son “ST_Slope” con un 100% y “ChestPainType” con un 48.69%. El modelo obtenido se ha clasificado con un 8.0% de errores, lo que se supone que hay un 92% de aciertos.

Además, se han obtenido una serie de reglas que el modelo ha acogido como decisivas para observar si una persona tiene riesgo de enfermedad del corazón. Todas estas se explican a continuación:



    1. Si una persona es de género femenino, tiene una presión arterial menor o igual que 148, un azúcar en sangre menor o igual a 0 y no tiene angina inducida por el ejercicio, no tendrá enfermedad del corazón con un 97.5%.
    
    2. Una persona con dolor de pecho NAP, una presión arterial menor o igual que 132, una máxima frecuencia cardíaca de 150, tendrá una probabilidad de no tener enfermedad del corazón con un 90.5%.
    
    3. Con una probabilidad del 87.8%, una persona con dolor de pecho ATA/TA y una máxima frecuencia cardíaca mayor que 150, no tednrá una enfermedad cardíaca.
    
    4. Una persona cuyo ST_Slope es Up, no tendrá una enfermedad del corazón con un 82.8%.
    
    5. Una persona con dolor de epcho ASY, una presión arterial menor o igual que 131, azúcar en sangre mayor que 0, tendrá una enfermedad cardíaca con una probabilidad de 98.1%.
    
    6. Con una probabilidad del 96%, una persona con dolor de pecho ASY, el colesterol menor o igual a 197 y con angina inducida por el ejercicio, tendrá una enfermedad del corazón.
    
    7. Una persona con dolor de pecho ASY, azúcar en sangre mayor que 0 y un Oldpeak de 0.6, tendrá enfermedad del corazon con un 95.2%.
    
    8. Con una probabilidad del 94.8%, un hombre con dolor de pecho  ASY, colesterol mayor que 227, azúcar en sangre menor o igual a 0, con dolor de angina inducida por el ejercicio, tendrá enfermedad del corazón.
    
    9. Un hombre con Oldpeak menor o igual a 0.3 y con ST_Slope down/flat, tendrá una enfermedad del corazón con un 93.9%.
    
    10. Un hombre con dolor de pecho ASY, azúcar en sangre menor o igual a 126, una máxima frecuencia cardíaca entre 146-160 y un ST_Slope Up, tendrá una enfermedad cardíaca con un 90.9%.
    
    11. Una persona con dolor de pecho ASY , azúcar en sangre menor o igual a 126, sin dolor de angina inducida por el ejercicio, con  Oldpeak mayor que  0.4 y ST_Slope  Up, tendrá enfermedad del corazón con un 90%.
    
    12. Una persona con ST_Slope down/flat, tendrá una enfermedad cardíaca con un 80.1%.

Viendo todo esto, se puede decir que una persona que no sufre dolor de angina inducida por el ejercicio, con azúcar en sangre menor o igual a 0 y una presión arterial menor o igual a 148, su posibilidad de terner enfermedades cardíacas será muy baja.


```{r,eval=TRUE,echo=TRUE}
predicted_model <- predict( modelo_clasificacion_C50, testx, type="class" )
print(sprintf("La precisión del modelo es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))

```

### Arbol de decisión con el método rpart

Se crea el árbol de decisión usando los datos de entrenamiento:

Utilizaremos dos bibliotecas diferentes, una de ellas es rpart, para crear el árbol de decisión y rpart.plot para representar visualmente nuestro árbol de decisión.

```{r,eval=TRUE,echo=TRUE}

modelo_clasificacion_rpart <- rpart(formula = trainy ~ ., data = trainx, method = "class" )

```

```{r,eval=TRUE,echo=TRUE}

summary(modelo_clasificacion_rpart)

```

Vamos a representar gráficamente el arbol obtenido de dos maneras diferentes:

```{r,eval=TRUE,echo=TRUE}

rpart.plot(modelo_clasificacion_rpart, type=2,extra = 2, under = TRUE, faclen=5,cex=.85) 
rpart.plot(modelo_clasificacion_rpart, type=2,extra = 8, under = TRUE, faclen=5,cex=.85) 

```

En el primer árbol Observamos las tasas de clasificación para cada nodo, expresada como el número de clasificaciones correctas y el número de observaciones en el nodo. En el segundo gráfico observamos la probabilidad por cada clase.


#### Predición del modelo

```{r,eval=TRUE,echo=TRUE}

predicted_model <- predict( modelo_clasificacion_rpart, testx, type="class" )
print(sprintf("La precisión del modelo es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))

```

Vamos a crear la matriz de confusión

```{r,eval=TRUE,echo=TRUE}

mat_conf_2<-table(testy,Predicted=predicted_model)
mat_conf_2

```

```{r,eval=TRUE,echo=TRUE}


CrossTable(testy, predicted_model,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))

```

```{r,eval=TRUE,echo=TRUE}

print(sprintf("La precisión del modelo4 es: %.4f %%",100*sum(predicted_model == testy) / length(predicted_model)))

```

Se aprecia que los verdaderos positivos son un 32% frente a un 6.2% de falsos positivos. Por lo tanto, tiene una alta sensibilidad.

Por otro lado, se observa que los verdaderos negativos es 51.6% frente a un 10.1% de falsos negativos, entonces se puede decir que tiene una alta especificidad.

### Modelo con mejora adaptative boosting


A continuación, utilizamos otro enfoque para comparar los resultados: incorporar como novedad “adaptative boosting”. La idea de esta técnica es generar varios clasificadores, con sus correspondientes arboles de decisión y su ser de reglas. Cuando un nuevo caso va a ser clasificado, cada clasificador vota cual es la clase predicha. Los votos son sumados y determina la clase final.

Además discretearemos el resto de variables continuas para intentar mejorar el modelo anterior , ya que los modelos de clasificación se compartan mejor con variables categóricas. Este proceso también nos servirá de utilidad a la hora de hacer representaciones gráficas más representativas.

Creamos un nuevo dataframe.


```{r,eval=TRUE,echo=TRUE}

heart_2 <-heart

head(heart_2)

```

Discretizamos el resto de variables:

Vamos a disctretizar nuevamente la **edad**, esta vez según criterios médicos. Teniendo en cuenta los criterios médicos y la distribución de los registros hemos decidido discretizarlos en los distintos grupos:

    0-39(Riesgo muy bajo)
    40-54(Riesgo bajo)
    55-64(Riesgo medio)
    65-100(Riesgo alto)
    
    
```{r,eval=TRUE,echo=TRUE}
heart_2["edad_discretizada"] <- cut(heart$Age, breaks = c(0,40,55,65,100), labels = c("0-39(Riesgo muy bajo)", "40-54(Riesgo bajo)", "55-64(Riesgo medio)", "65-100(Riesgo alto)"))
```


Variable **RestingBP**T, Presión arterial:

Para esta variable también tendremos en cuenta las categorías establecidas por los cardiólogos:

    Normal, menos de 120 
    Elevada: 120-129
    Presión arterial alta nivel 1: 130-139
    Presión arterial alta nivel 2: 140-179
    Crisis de hipertensión: mayor de 180

```{r,eval=TRUE,echo=TRUE}

heart_2["restingBP_discretizado"] <- cut(heart$RestingBP , breaks = c(0,120,130,140,180,500), labels = c("0-119(Normal)", "120-129(Elevada)", "130-139(Alta nivel 1)", "140-179(Alta nivel 2)", "180-x(Crisis de hipertesión)"))

```

Variable **coresterol**, en este caso vamos a discretizar la variable únicamente siguiendo criterios médicos, ya que han sido rigurosamente establecidos.

Se considera deseable un nivel de colesterol menor de 200 mg/dl, alto entre 200 y 239 y muy alto mayor de 240 mg/dl

```{r,eval=TRUE,echo=TRUE}

heart_2["chol_discretizado"] <- cut(heart$Cholesterol, breaks = c(-1,200,240,1000), labels = c("0-199(Riesgo bajo)", "200-239(Riesgo medio)", "240-x(Riesgo alto)"))

```

Variable **FastingBS**, un nivel de glucosa sanguínea en ayunas por debajo de 100 miligramos por decilitro (mg/dl) (5,6 milimoles por litro [mmol/l]) se considera normal. Un nivel de glucosa sanguínea en ayunas entre 100 y 125 mg/dL (5,6 a 7,0 mmol/L ) se considera prediabetes. En este caso, tenemos una variable binaria. 0 representa normal y 1 alta. factorizamos dicha variable. 

```{r,eval=TRUE,echo=TRUE}
heart_2$FastingBS <- as.factor(heart_2$FastingBS)

```

```{r,eval=TRUE,echo=TRUE}


summary(heart[,"FastingBS"])

```

Variable **MaxHR**, no tiene sentido discretizar esta variable.

Variable **OldPeak**:

```{r,eval=TRUE,echo=TRUE}
summary(heart[,"Oldpeak"])
```

```{r,eval=TRUE,echo=TRUE}

heart_2["oldpeak_discretizado"] <- cut(heart$Oldpeak, breaks = c(-3,0,2,4,6,8,10), labels = c(0, 1, 2, 3,4,7))

```

Factorizamos la variable que queremos clasificar, HeartDisease:

```{r,eval=TRUE,echo=TRUE}
heart_2$HeartDisease <- as.factor(heart_2$HeartDisease)

```

Vamos a fijarnos el análisis PCA anterior para ordenar dichas variables según su importancia en el modelo y de esta manera usar para el modelo de clasificación aquellas variables que maximicen el resultado.

```{r,eval=TRUE,echo=TRUE}
fviz_contrib(pca, addlabels = TRUE, choice = "var", axes = 1, top = 20)
```
Ordenamos las variables:

```{r,eval=TRUE,echo=TRUE}
library(dplyr)
heart_2 = heart_2 %>% select(ST_Slope, ExerciseAngina , MaxHR, Oldpeak, Age, ChestPainType, Sex, FastingBS, RestingBP, RestingECG, Cholesterol,  oldpeak_discretizado, edad_discretizada, restingBP_discretizado, chol_discretizado, HeartDisease)


head(heart_2)

```

Ya tenemos el dataframe que podremos utilizar en el modelo.


```{r,eval=TRUE,echo=TRUE}

set.seed(1)
heart_clasification_2 <- heart_2[sample(nrow(heart_2)),]
```

En este caso utilizaremos todas las variables, 15, para clasificar la variable **HeartDisease**, 16. 

```{r,eval=TRUE,echo=TRUE}

set.seed(666)
y <- heart_clasification_2[,16] 
X <- heart_clasification_2[,1:15] 

```

Dividimos el dataset en dos partes diferenciadas, train y test. Podemos crear directamente un rango utilizando el parámetro split_prop

```{r,eval=TRUE,echo=TRUE}

split_prop <- 3 
indexes = sample(1:nrow( heart_clasification_2), size=floor(((split_prop-1)/split_prop)*nrow(heart_clasification_2)))
trainx2<-X[indexes,]
trainy2<-y[indexes]
testx2<-X[-indexes,]
testy2<-y[-indexes]

```

Creamos el modelo con técnicas de Boosting, 100 intentos:

```{r,eval=TRUE,echo=TRUE}

modelo_trials_2 <- C50::C5.0(trainx2, trainy2,rules=TRUE, trials = 100,control = C5.0Control(noGlobalPruning = FALSE) )

```


Ahora, calculamos la precisión del modelo a partir de la matriz de confianza.


```{r,eval=TRUE,echo=TRUE}

predicted_model_2 <- predict( modelo_trials_2, testx2, type="class" )
mat_conf_2<-table(testy2,Predicted=predicted_model_2)

porcentaje_correct_2<-100 * sum(diag(mat_conf_2)) / sum(mat_conf_2)
print(sprintf("El %% de registros correctamente clasificados es: %.4f %%",porcentaje_correct_2))

```

Se aprecia que la precisión con trials es  82.3529 %.

Finalmente, se crea la matriz de confusión:


```{r,eval=TRUE,echo=TRUE}

CrossTable(testy2, predicted_model_2,prop.chisq  = FALSE, prop.c = FALSE, prop.r =FALSE,dnn = c('Reality', 'Prediction'))

```
```{r,eval=TRUE,echo=TRUE}

modelo_clasificacion_rpart_2 <- rpart(formula = trainy2 ~ ., data = trainx2, method = "class" )
rpart.plot(modelo_clasificacion_rpart_2, type=2,extra = 8, under = TRUE, faclen=5,cex=.85) 

```

Realizar el nuevo modelo con boosting de 100 intentos y añadiendo las variables discretizadas lamentablemente no ha mejorado la precisión del modelo. Por otro lado, hemos podido obtener un conjunto de datos más completo y ordenado y además estas nuevas variables discretas nos permitirán a continuación realizar un análisis gráfico más eficiente. 


# Visualización

Vamos a realizar diferentes estudios gráficos para analizar las distintas variables del modelo y las relaciones entre ellas.

Para ello, diferenciaremos entre los tipos de variables continuas y discretas para su análisis y sí fuera necesario transformaremos las variables para su estudio más eficiente. 

```{r}
library(ggplot2)
ggplot(data=heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar() + ggtitle("Tipo de trastorno por edad(G1)") + labs(x="Edad", colour= "Diagnóstico") 

ggplot(data=heart_2,aes(x=chol_discretizado,fill=HeartDisease))+geom_bar() + ggtitle("Tipo de trastorno por coresterol(G2)") + labs(x="Nivel de Coresterol", colour= "Diagnóstico")

ggplot(data=heart_2,aes(x=restingBP_discretizado, fill=HeartDisease))+geom_bar() + ggtitle("Tipo de trastorno por presión arterial(G3)") + labs(x="Presión Arterial", colour= "Diagnóstico")  + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

ggplot(data=heart_2,aes(x=oldpeak_discretizado,fill=HeartDisease))+geom_bar() + ggtitle("Tipo de trastorno por oldpeak (G4)") + labs(x="Oldpeak", colour= "Diagnóstico")
```
Observando los gráficos anteriores podemos llevar a cabo las siguientes conclusiones:

G1 - La variable edad influye en el riesgo de padecer enfermedades cardíacas o no, cuanto mayor es la edad del paciente mayor es la probabilidad de sufrir una enfermedad cardíaca.

G2 - Cuanto mayor es el nivel de colesterol más riesgo existe de sufrir estas enfermedades aunque no es una variable decisiva.

G3 - La presión arterial es un factor de riego importante para prevenir estas enfermedades, siendo mayor de 140 un factor crítico.

G4 - Sin duda, gráficamente podemos observar que la variable discretizada más relevante es oldpeak, cuando la depresión del ST es mayor que cero el riesgo es crítico



A continuación y para completar el análisis gráfico vamos a realizar el análisis gráfico de las variables más relevantes en referencia al análisis PCA y si tienen riesgo de sufirir o no un ataque cardíaco en relación con la edad:


```{r}
ggplot(data = heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar(position="fill")+facet_wrap(~Sex)+ theme(text = element_text(size=8)) + ggtitle("Diagnóstico por Edad y Sexo (G5)") + labs(x="Gender", y="", colour= "Diagnóstico") + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

ggplot(data = heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar(position="fill")+facet_wrap(~ChestPainType)+ theme(text = element_text(size=8)) + ggtitle("Diagnóstico por Edad y Tipo de dolor de pecho (G6)") + labs(x="Tipo de dolor", y="", colour= "Diagnóstico") + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

ggplot(data = heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar(position="fill")+facet_wrap(~ST_Slope)+ theme(text = element_text(size=8)) + ggtitle("Diagnóstico por Edad y pendiente del pico del segmento (G7)") + labs(x="Edad", y="", colour= "Diagnóstico") + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

ggplot(data = heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar(position="fill")+facet_wrap(~oldpeak_discretizado)+ theme(text = element_text(size=8)) + ggtitle("Diagnóstico por Edad y depresión del oldpeak (G8)") + labs(x="Edad", y="", colour= "Diagnóstico") + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

ggplot(data = heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar(position="fill")+facet_wrap(~ExerciseAngina)+ theme(text = element_text(size=8)) + ggtitle("Diagnóstico por Edad y angina inducida por el ejercicio 1:si 0:no (G9)") + labs(x="Edad", y="", colour= "Diagnóstico") + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

ggplot(data = heart_2,aes(x=edad_discretizada,fill=HeartDisease))+geom_bar(position="fill")+facet_wrap(~restingBP_discretizado)+ theme(text = element_text(size=8)) + ggtitle("Diagnóstico por Edad y Presión arterial en reposo (G10)") + labs(x="Edad", y="", colour= "Diagnóstico") + theme(text = element_text(size=12),  axis.text.x = element_text(angle=90, hjust=1))

```


G5 - Los hombres tienen más probabilidad de sufrir enfermedades cardíacas y esta aumenta conforme su edad aumenta.

G6 - Aquellas personas asintomáticas, seguidas de un dolor no anginoso son las más vulnerables. No tener síntomas de este tipo, no garantiza no sufrir una enfermedad cardíaca

G7 - Observando la relación entre ST_Slope y la edad, concluimos que una pendiente del pico del segmento ST, plana es sinónimo de enfermedad cardíaca para todos los grupos de edad y una pendiente descendente también confirma un riesgo de enfermedad cardíaca, especialmente para las edades más avanzadas.

G9 - La variable oldpeak, es determinante, para valores mayores que uno el riesgo para todas las edades es mayor al 50%, si la depresión es mayor o igual que 2 estamos hablando de un riesgo mayor que el 70%.

G9 - Si la angina ha sido enducida por el ejercicio, el riesgo de padecer una enfermedad cardíaca es muy alta para todas las edades, incrementándose cuando esta aumenta.

G10 - La variable presión arterial en reposo es importante a la hora de padecer enfermedades cardíacas en edades superiores a los 50 años y conforme la edad aumenta y la presión arterial también lo hace este riesgo es mayor.


# Exportamos el conjunto de datos

```{r}
write.csv(heart_2,"../Data/heart_out.csv", row.names = FALSE)
```

# Conclusiones

Durante este proyecto hemos introducido todos aquellos puntos más relevantes estudiados en la asignatura. Para ello elegimos el conjunto de datos referente al estudio de aquellas variables relevantes a la hora de sufrir o no una enfermedad cardiovascular. Lo elegimos por varias razones, la primera: nos resultaba interesante ya que es este tipo de enfermedades es la mayor causa de mortalidad en el mundo y gracias a técnicas más avanzadas de machine learning pensamos que se podrían salvar muchas vidas. Por otro lado, el conjunto de datos disponía de un tipo variado de datos con los que poder trabajar. El conjunto de datos estaba bastante limpio de tal manera que hemos tenido más énfasis en la parte del análisis, aún así, hemos implementado las técnicas estudiadas. 

Dividimos el proyecto en dos partes: la primera limpieza y preporcesado y la segunda el análisis de los datos.

En concreto en el proceso de limpieza y preporcesado hemos realizado las siguientes tareas:

    - Conversión de cada variable al tipo de dato más adecuado.
    - Eestudio de Outliers.
    - Reazidado un estudio de los valores nulos.
    - Sustitución de los valores nulos mediante el algoritmo de KNN
    - Análisis de los componentes principales: PCA.
    - Normalización min-max, de la variable edad (Para prácticar con esta técnica).
    - Discretización de la variable edad (Para prácticar con esta técnica).

En la segunda parte, al igual que en la primera hemos realizado los distintos procesos estudiados en la asignatura:

    - Estudio descriptivo de las variables.
    - Estudio de normalidad de las variables.
    - Contraste de hipótesis.
    - Estudio de correlación entre las variables.
    - Regresión lineal
    - Regresión lógistica
    - Modelos de clasificación: Árboles de decisón.
    
Gracias a estos análisis hemos llegado a las siguientes conclusiones, ordenadas según los análisis comentados previamente:

    - Las mujeres tienen más colesterol que los hombres. 
    - Los hombres con enfermedad cardíaca tienen el mismo nivel de azúcar en sangre que las mujeres con enfermedad cardíaca.
    - Las variables que tienen más correlación con padecer o no una enfermedad cardíaca son:  MaxHR, ExerciseAngina,Olpeak y ST_Slope.
    - Hemos creado un modelo de regresión logistica múltiple capaz de clasificar los datos con un 85.87% de efectividad.
    - Hemos creado distintas reglas de decisión a través de un modelo de clasificación con arboles de decisión.
    - Este modelo predice los datos con una precisón de 85.3%.
    


Para completar el trabajo, hemos realizado un análisis gráfico de la relación entre las distintas variables categorizadas, distinguiendo por grupos de edad y riesgo de sufrir una enfermedad cardíaca o no.


******

# Contribuciones

| **Contribuciones**          | **Firma**                          |
|-----------------------------|------------------------------------|
| Investigación previa        | Brais Santos, Jorge Álvarez Gracia |
| Redacción de las respuestas | Brais Santos, Jorge Álvarez Gracia |
| Desarrollo código           | Brais Santos, Jorge Álvarez Gracia |

# Recursos

Enlace Vídeo: 

Enlace repositorio GitHub: https://github.com/bsantosn/LIMPIEZA_DATOS.git

******

******